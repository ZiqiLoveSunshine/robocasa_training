_wandb:
    value:
        cli_version: 0.24.0
        e:
            ux6keji6ir9ktzn46q0ug7xblhnbvqm2:
                args:
                    - --env_name
                    - PnPCounterToCab
                    - --max_timesteps
                    - "500000"
                    - --headless
                    - --wandb
                codePath: train.py
                codePathLocal: train.py
                email: ziqi_ma0605@163.com
                executable: /home/philippe/anaconda3/envs/robocasa/bin/python
                host: Philippe-U2IS-server1
                os: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
                program: /home/philippe/Documents/robocasa_training/train.py
                python: CPython 3.10.19
                root: /home/philippe/Documents/robocasa_training
                startedAt: "2026-01-30T14:16:28.823300Z"
                writerId: ux6keji6ir9ktzn46q0ug7xblhnbvqm2
        m: []
        python_version: 3.10.19
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 13
                - 16
                - 35
            "4": 3.10.19
            "5": 0.24.0
            "12": 0.24.0
            "13": linux-x86_64
close_environment_at_exit:
    value: true
disable_progressbar:
    value: false
discount_factor:
    value: 0.9
entropy_loss_scale:
    value: 0
environment_info:
    value: episode
experiment:
    value:
        checkpoint_interval: 2000
        directory: runs/robocasa/PnPCounterToCab_visual
        experiment_name: ""
        store_separately: false
        wandb: true
        wandb_kwargs:
            entity: Graph-Mobile-Manipulator
            monitor_gym: true
            name: robocasa_PnPCounterToCab
            project: robocasa_experiments
        write_interval: 200
grad_norm_clip:
    value: 0.5
headless:
    value: true
kl_threshold:
    value: 0
lambda_:
    value: 0.95
learning_epochs:
    value: 10
learning_rate:
    value:
        - 0.0005
        - 0.0005
learning_rate_scheduler:
    value:
        - skrl.resources.schedulers.torch.kl_adaptive.KLAdaptiveLR
        - skrl.resources.schedulers.torch.kl_adaptive.KLAdaptiveLR
learning_rate_scheduler_kwargs:
    value:
        - kl_threshold: 0.008
        - kl_threshold: 0.008
learning_starts:
    value: 0
mini_batches:
    value: 32
mixed_precision:
    value: false
observation_preprocessor:
    value: null
policy:
    value:
        "0": Linear(in_features=256, out_features=128, bias=True)
        "1": ReLU()
        "2": Linear(in_features=128, out_features=12, bias=True)
        "3": Tanh()
random_timesteps:
    value: 0
ratio_clip:
    value: 0.2
render_interval:
    value: 1
rewards_shaper:
    value: null
rollouts:
    value: 2048
state_preprocessor:
    value: null
stochastic_evaluation:
    value: false
time_limit_bootstrap:
    value: false
timesteps:
    value: 500000
value:
    value:
        "0": Linear(in_features=256, out_features=128, bias=True)
        "1": ReLU()
        "2": Linear(in_features=128, out_features=1, bias=True)
value_clip:
    value: 0.2
value_loss_scale:
    value: 0.5
value_preprocessor:
    value: null
